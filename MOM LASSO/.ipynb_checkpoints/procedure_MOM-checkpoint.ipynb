{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as alea\n",
    "import random as rd\n",
    "from math import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MOM(X , K) :\n",
    "    \n",
    "    #X: np.array\n",
    "    #K: int\n",
    "    #return: Median of means of the K blocks with the indexes of the values inside the median block ((float,int list))\n",
    "    n = len(X)\n",
    "    j = n // K\n",
    "    idx = alea.permutation(n)\n",
    "    means_blocks = np.zeros(K)\n",
    "    \n",
    "    for i in range(K) :\n",
    "        \n",
    "        means_blocks[i] = np.mean(X[idx[ j * i : j * (i + 1)]])\n",
    "        \n",
    "    indices = np.argsort(means_blocks)[int(np.ceil( len(means_blocks) / 2 ))]\n",
    "    \n",
    "    return (np.median(means_blocks[indices]) , idx[j * indices : j * (indices + 1)])\n",
    "\n",
    "def lt_lt_prime(X , Y , t , t_prime) :\n",
    "    \n",
    "    #X: predictor np.matrix\n",
    "    #Y: target np.array\n",
    "    #t,t_prime: coefs (np.array)\n",
    "    #return: (Y,Xt)**2-(Y,Xt_prime)**2 (np.array)\n",
    "    \n",
    "    n , _ = np.shape(Y)\n",
    "    \n",
    "    Nt = Y - X @ t\n",
    "    Nt_prime = Y - X @ t_prime\n",
    "    TABL = (np.square(Nt , Nt) - np.square(Nt_prime , Nt_prime)).flatten()\n",
    "    \n",
    "    print(TABL)\n",
    "    \n",
    "    return TABL[0]\n",
    "\n",
    "def subgrad(X , Y , t , lamb) :\n",
    "    \n",
    "    #X: data base (matrix)\n",
    "    #Y: results (array)\n",
    "    #t: linear approximation of Y with X (array)\n",
    "    #lamb: int\n",
    "    #return: subgradient of the lagrangian of the lasso evaluated in t (array)\n",
    "    \n",
    "    return -2 * (X.T) @ (Y - X @ t) + lamb * np.sign(t)\n",
    "\n",
    "def grad(X , Y , t) :\n",
    "    \n",
    "    #X: data base (matrix)\n",
    "    #Y: results (array)\n",
    "    #t: linear approximation of Y with X (array)\n",
    "    #return: gradient of the least square regression evaluated in t (array)\n",
    "    \n",
    "    return -2 * (X.T) @ (Y - X @ t)\n",
    "\n",
    "def norm1(X) :\n",
    "    \n",
    "    return np.sum(np.abs(X))\n",
    "\n",
    "def F(X , Y , t , lamb) :\n",
    "    \n",
    "    #X: data base (matrix)\n",
    "    #Y: results (array)\n",
    "    #t: linear approximation of Y with X (array)\n",
    "    #lamb: int\n",
    "    #return: lagrangian of the lasso\n",
    "    \n",
    "    return ((Y - X @ t).T) @ (Y - X @ t) + lamb * norm1(t)\n",
    "\n",
    "def som(t , X , deb , fin) :\n",
    "    \n",
    "    #t= (array)\n",
    "    #X: data base (matrix)\n",
    "    #deb: index of the first element of the sum\n",
    "    #fin: index of the last element of the sum\n",
    "    #return: (array)\n",
    "    \n",
    "    c = 0\n",
    "    \n",
    "    for i in range(deb,fin) :\n",
    "        \n",
    "        c += int(t[i][0]) * X[ :,i]\n",
    "        \n",
    "    return c\n",
    "\n",
    "def part_pos(x) :\n",
    "    \n",
    "    return max(0,x)  \n",
    "    \n",
    "def P_quadra(X,Y,t) : \n",
    "    \n",
    "    #X: data base (matrix)\n",
    "    #Y: results (array)\n",
    "    #t: linear approximation of Y with X (array)\n",
    "    #return: quadratic error between Yi and (Xt)i (float list)\n",
    "    \n",
    "    Nt = Y - X @ t\n",
    "    \n",
    "    return np.square(Nt).flatten()\n",
    "\n",
    "def soft_thresholding(lamb , t) :\n",
    "    \n",
    "    #lamb: (float)\n",
    "    #t: (array)\n",
    "    #return soft thresholding of t-lamb and 0\n",
    "    \n",
    "    P = np.zeros((len(t),1))\n",
    "    \n",
    "    for i in range(len(t)) :\n",
    "        \n",
    "        P[i][0] = np.sign(t[i][0]) * max(abs(t[i][0]) - lamb , 0)\n",
    "        \n",
    "    return P\n",
    "\n",
    "def quadra_loss(X , Y , t) :\n",
    "    \n",
    "    #X: data base (matrix)\n",
    "    #Y: results (array)\n",
    "    #t: linear approximation of Y with X (array)\n",
    "    #return: quadratic error between Y and Xt (float)\n",
    "    \n",
    "    return np.linalg.norm(Y - X @ t) ** 2\n",
    "\n",
    "def diviseurs(n) :\n",
    "    \n",
    "    l1 = []\n",
    "    l2 = []\n",
    "    \n",
    "    for i in range(1,int(abs(np.sqrt(n))) + 1) :\n",
    "        \n",
    "        if n % i == 0 :\n",
    "            \n",
    "            l1.append(i)\n",
    "            l2.append(n // i)\n",
    "            \n",
    "    l2.reverse()\n",
    "    \n",
    "    return l1 + l2\n",
    "\n",
    "def min_pos(tabl) :\n",
    "    \n",
    "    #tabl: float array\n",
    "    #return: min of the positive elements in tabl (float)\n",
    "    \n",
    "    mini = inf\n",
    "    ind = -1\n",
    "    n,p = np.shape(tabl)\n",
    "    \n",
    "    for i in range(p) :\n",
    "        \n",
    "        if tabl[0,i] < mini and tabl[0,i] > 1e-10 :\n",
    "            \n",
    "            mini = tabl[0,i]\n",
    "            ind = i\n",
    "            \n",
    "    return mini,ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
